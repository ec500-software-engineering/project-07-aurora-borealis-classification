<!DOCTYPE html>
<html lang="no"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta name="description" content="">
    <title>OATH Training Dataset</title>
    <link rel="shortcut icon" href="http://www.uio.no/vrtx/decorating/resources/dist/images/favicon.ico">
    <link rel="stylesheet" media="all" href="OATH%20Training%20Dataset_files/style.css">
    <link rel="stylesheet" href="OATH%20Training%20Dataset_files/frontpage-override-new.css" type="text/css" media="all">
		<style>
			pre {
				font-style: monospace;
				font-style: normal;
				font-variant: normal;
				font-weight: 400;
			}
			samp {
				font-size: 11pt;
			}
			code {
				font-size: 10pt;
  			display: block;
  			white-space: pre-wrap   
			}
		</style>
  </head>
  <body class="www.uio.no not-for-ansatte faculty no total-main has-slogan vrtx-frontpage-full-width" id="vrtx-frontpage-full-width">
		<div style="text-align: center; width: 600px; margin: 0 auto;">
			<table style="">
				<tbody><tr>
					<td>
						<h2>
					    Oslo Aurora THEMIS (OATH) Training Dataset
						</h2>
					</td>
				</tr>
				<tr>
					<td>
<h3>Background</h3>

Clausen &amp; Nickisch<a href="#ref">[1]</a> showed that relatively 
standard, off-the-shelf machine learning tools can be used to 
effectively and automatically classify auroral images. On this website 
you will find information about the tools and the training dataset used,
 and the code with which you can replicate the results.<p>

In Clausen &amp; Nickisch<a href="#ref">[1]</a> the following auroral classification was introduced:<br>
</p><table>
<tbody><tr>
<th>Label</th><th>Explanation</th><th>class6</th><th>class2</th>
</tr>
<tr>
<td>arc</td><td>This label is used for images that show one or multiple 
bands of aurora that stretch across the field-of-view; typically, the 
arcs have well-defined, sharp edges.</td>
<td style="text-align: center;">0</td><td rowspan="3" style="text-align: center;">0</td>
</tr>
<tr>
<td>diffuse</td><td>Images that show large patches of aurora, typically 
with fuzzy edges, are placed in this category. The auroral brightness is
 on the order of that of stars.</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td>discrete</td><td>The images show auroral forms with well-defined, 
sharp edges, that are, however, not arc-like. The auroral brightness is 
high compared to that of stars.</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td>cloudy</td><td>The sky in these images is dominated by clouds or the dome of the imager is covered with snow.</td>
<td style="text-align: center;">3</td><td rowspan="3" style="text-align: center;">1</td>
</tr>
<tr>
<td>moon</td><td>The image is dominated by light from the Moon.</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td>clear/noaurora</td><td>This label is attached to images which show a
 clear sky (stars and planets are clearly visible) without the 
appearance of aurora.</td>
<td style="text-align: center;">5</td>
</tr>
</tbody></table>

<h3>Download</h3>

<a href="http://tid.uio.no/plasma/oath/oath_v1.1_20181026.tgz">You can download the training dataset together with the Python3 code that trains the ridge classifier here (about 500MB)</a>. It is a tar archive (<a href="http://tid.uio.no/plasma/oath/SHA256SUM_v1.1">SHA256SUM here</a>) that, once unpacked, creates the following directory structure and files:
<pre>oath/
  |
  +- 00_README
  |
  +- classification/
  |   |
  |   +- classification.csv
  |   |
  |   +- train_test_split.csv
  |
  +- code/
  |   |
  |   +- ridge.py
  |   |
  |   +- rotate.sh
  |
  +- features/
  |   |
  |   +- auroral_feat.h5
  |
  +- images/
      |
      +- cropped_scaled/
      |   |
      |   +- 00001.png
      |   |
      |   +- 00002.png
      |   |
      |   +- ...
      |   |
      |   +- 05824.png
      |
      +- files_origin.csv</pre>

<table>
<tbody><tr>
<td><samp>00_README</samp></td>
<td>A text file containing this installation information and lisense information</td>
</tr>
<tr>
<td><samp>classification.csv</samp></td>
<td>Each line of this file contains information about the image files: 
numeric class (2 classes), numeric class (6 classes), image index 
number, label, rotation angle</td>
</tr>
<tr>
<td><samp>train_test_split.csv</samp></td>
<td>This files contains 5 lines of each 5824 elements. These elements 
are the randomized index numbers of the images. As can be seen from <samp>ridge.py</samp>,
 the contents of the file can be used for the splitting of the annotated
 dataset into a training and a test dataset. Including the indeces for 
each dataset makes it easier in the future to compare the preformance of
 different maschines. </td>
</tr>
<tr>
<td><samp>ridge.py</samp></td>
<td>Python code that trains a ridge classifier using the feature vectors extracted from all images of the training dataset</td>
</tr>
<tr>
<td><samp>rotate.sh</samp></td>
<td>A bash script that rotates the original images from the <samp>oath/images/cropped_scaled</samp> folder and places them in a new folder called <samp>oath/images/cropped_scaled_rotated</samp>.</td>
</tr>
<tr>
<td><samp>auroral_feat.h5</samp></td>
<td>HDF5 file containing the feature vectors for the training dataset</td>
</tr>
<tr>
<td><samp>files_origin.csv</samp></td>
<td>Each line of this file contains the original source of each image in
 the training dataset: the THEMIS ASI station abbreviation, the date and
 time the image was taken, and its file path in the <samp>oath</samp> directory which contains the image index number (00001, 00002, etc)</td>
</tr>
<tr>
<td><samp>00001.png</samp></td>
<td>THEMIS ASI image, cropped and scaled</td>
</tr>
</tbody></table>

<h3>Installation</h3>

Here we describe the installation of the necessary components to 
replicate the training of a ridge classifier based on auroral feature 
detection as described in Clausen &amp; Nickisch [1]. This installation 
was tested on a fresh install of <a href="http://releases.ubuntu.com/17.10.1/ubuntu-17.10.1-desktop-amd64.iso">Ubuntu</a>
 17.10 (Artful Aardvark), Kernel 4.13.0-37 generic, x86_64, running on a
 laptop with a four-core Intel Core i7-3520-M CPU (2.9 GHz).<p>

In the following examples the OATH tarball <a href="https://xkcd.com/1168/" target="_new">was extracted</a> in the user's home directory <samp>~/</samp>.

</p><ol>

<li>Make sure Python3, git, wget, and imagemagick are installed
	<pre>sudo apt install python3 git wget imagemagick</pre>
</li>

<li>Install several Python3 packages for TensorFlow™
	<pre>sudo apt install python3-pip python3-dev python3-h5py python3-contextlib2</pre>
</li>

<li>Install several Python3 packages for machine learning
	<pre>sudo apt install python3-matplotlib python3-pandas python3-sklearn</pre>
</li>

<li>Install <a href="https://www.tensorflow.org/" target="_new">TensorFlow</a>™
	<pre>mkdir ~/tensorflow/
cd ~/tensorflow
pip3 install tensorflow
git clone https://github.com/tensorflow/models/
cd models/research/slim
sudo python3 setup.py install</pre>
</li>

<li>Download pre-trained <a href="https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models" target="_new">Inception model checkpoint</a>
	<pre>cd ~/tensorflow
mkdir checkpoints
cd checkpoints
wget http://download.tensorflow.org/models/inception_v4_2016_09_09.tar.gz
tar xf inception_v4_2016_09_09.tar.gz</pre>
</li>

<li>Install <a href="https://github.com/tomrunia/TF_FeatureExtraction" target="_new">TF_FeatureExtraction</a>
	<pre>cd ~/tensorflow
git clone https://github.com/tomrunia/TF_FeatureExtraction</pre>

</li></ol>

<h3>Running the feature detection</h3>

This part assumes that the cropped, and scaled images are in the folder <samp>~/oath/images/cropped_scaled</samp> (see download above). First, the images are rotated and placed in the folder <samp>~/oath/images/cropped_scaled_rotated</samp>. Then, the TF_FeatureExtraction extracts the feature vectors and writes them into a HDF5 file called <samp>auroral_feat.h5</samp> in the directory <samp>~/oath/features/</samp>. On the laptop mentioned above (four-core Intel Core i7-3520-M) this takes about one hour.

<ol>
<li>Rotate the images
	<pre>cd ~/oath/code
chmod a+x rotate.sh
./rotate.sh</pre>
</li>

<li>Run feature extraction
	<pre>cd ~/tensorflow/TF_FeatureExtraction
# this is one long command
python3 example_feat_extract.py --network inception_v4 --checkpoint ../checkpoints/inception_v4.ckpt 
	--image_path ~/oath/images/cropped_scaled_rotated/ --out_file ~/oath/features/auroral_feat.h5 
	--layer_names Logits</pre>
</li>

<li>Train the ridge classifier
	<pre>cd ~/oath/code
python3 ridge.py</pre>
</li>

<li>This should produce the following output:
	<pre>0.8174012593016601 0.010271527444147862
[[139  27  66   0   0  14]
 [ 37 222  58   1   0  16]
 [ 36  37 335   3   0   3]
 [  0   1   5 224   3   2] 
 [  0   1   3   2 183   1]
 [  7  13   7   1   1 299]]</pre>
</li>

</ol>
					</td>
				</tr>
				<tr>
					<td>
<h3>Comments &amp; questions</h3>
Comments and questions can be directed to <a href="mailto:lasse.clausen@fys.uio.no">Lasse Clausen</a>
<h3>References</h3>
<a name="ref">If you use the Oslo Auroral THEMIS dataset, please refer to:</a><p>
Clausen, L. B. N., &amp; Nickisch, H. (<b>2018</b>). Automatic classification of auroral images from the Oslo Auroral THEMIS (OATH) data set using machine learning. <i>Journal of Geophysical Research: Space Physics</i>, 123, <a href="https://doi.org/10.1029/2018JA025274" target="_new">https://doi.org/10.1029/2018JA025274</a>
</p><h3>Acknowledgements &amp; copyright</h3>
Unless stated otherwise, all data in the OATH Dataset is licensed under a <a href="https://creativecommons.org/licenses/by/4.0/">Creative Commons 4.0 Attribution License (CC BY 4.0)</a> and the accompanying source code is licensed under a <a href="https://opensource.org/licenses/BSD-2-Clause">BSD-2-Clause License</a>.<p>

In particular, all actual image data included in the tarball are modified from the <a href="https://www.nasa.gov/mission_pages/themis/spacecraft/asi.html">THEMIS all-sky imagers</a>. We thank H. Frey for giving us permission to include these data. Copyright for these data remains with NASA.</p><p>

We acknowledge NASA contract NAS5-02099 and V. Angelopoulos for use of 
data  from the THEMIS Mission. Specifically: S. Mende and E. Donovan for
 use of the ASI data, the CSA for logistical support in fielding and 
data retrieval from the GBO stations, and NSF for support of GIMNAST 
through grant AGS-1004736.
					</p></td>
				</tr>
			</tbody></table>
		</div>
  

</body></html>